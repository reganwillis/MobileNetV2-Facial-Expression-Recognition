{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":1351797,"datasetId":786787,"databundleVersionId":1384195,"isSourceIdPinned":false}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# download dataset to file\n\nimport kagglehub\n# Download latest version\npath = kagglehub.dataset_download(\"msambare/fer2013\")\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T15:31:38.887241Z","iopub.execute_input":"2025-06-02T15:31:38.888251Z","iopub.status.idle":"2025-06-02T15:31:39.569491Z","shell.execute_reply.started":"2025-06-02T15:31:38.888199Z","shell.execute_reply":"2025-06-02T15:31:39.568496Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/fer2013\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T15:31:43.086803Z","iopub.execute_input":"2025-06-02T15:31:43.087091Z","iopub.status.idle":"2025-06-02T15:31:48.058380Z","shell.execute_reply.started":"2025-06-02T15:31:43.087069Z","shell.execute_reply":"2025-06-02T15:31:48.057357Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from torchinfo import summary\nfrom transformers.models.mobilenet_v2 import MobileNetV2Model\nfrom transformers.models.mobilenet_v2 import MobileNetV2PreTrainedModel\nfrom transformers.models.mobilenet_v2.configuration_mobilenet_v2 import MobileNetV2Config","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T14:54:47.299778Z","iopub.execute_input":"2025-06-02T14:54:47.300264Z","iopub.status.idle":"2025-06-02T14:54:47.305736Z","shell.execute_reply.started":"2025-06-02T14:54:47.300232Z","shell.execute_reply":"2025-06-02T14:54:47.304767Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# constants\nPRETRAINED_CONFIG = ''\n# https://huggingface.co/google/mobilenet_v2_1.4_224\nPRETRAINED_MODEL = 'google/mobilenet_v2_1.4_224'\n#cfg = MobileNetV2Config(PRETRAINED_CONFIG)\ncfg = MobileNetV2Config()\nBATCH_SIZE = 16\nNUM_EPOCHS = 1\n\nif not torch.cuda.is_available():\n    print('GPU not available, running script on CPU..')\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T14:55:16.239662Z","iopub.execute_input":"2025-06-02T14:55:16.240009Z","iopub.status.idle":"2025-06-02T14:55:16.247136Z","shell.execute_reply.started":"2025-06-02T14:55:16.239984Z","shell.execute_reply":"2025-06-02T14:55:16.245737Z"}},"outputs":[{"name":"stdout","text":"GPU not available, running script on CPU..\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"class FERDataset(torch.utils.data.Dataset):\n    # https://www.kaggle.com/datasets/msambare/fer2013\n    def __init__(self, images, labels):\n        self.images = images\n        self.labels = labels\n        self.transform = self._transform\n        self.target_transform = self._target_transform\n        self.class_map_loaded = {\n            0: 'angry',\n            1: 'disgust',\n            2: 'fear',\n            3: 'happy',\n            4: 'neutral',\n            5: 'sad',\n            6: 'surprise'\n        }\n        self.class_map = {\n            0: 'positive',\n            1: 'negative',\n            2: 'neutral'\n        }\n        self.NUM_CLASSES = len(self.class_map)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        image = self.transform(image)\n        label = self.labels[idx]\n        label = self.target_transform(label)\n\n        return image, label\n\n    def _transform(self, image):\n        return image\n\n    def _target_transform(self, target):\n        # TODO: map to new class map\n        return target","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load dataset into script\nprint('loading dataset from file...')\n\ntrain_images = []\ntrain_labels = []\nfor subdir in os.listdir(path+'/train'):\n    for idx in os.listdir(path+'/train/'+subdir):\n        train_images.append(idx)\n        train_labels.append(subdir)\n\ntest_images = []\ntest_labels = []\nfor subdir in os.listdir(path+'/test'):\n    for idx in os.listdir(path+'/test/'+subdir):\n        train_images.append(idx)\n        train_labels.append(subdir)\nprint('...done')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T15:41:40.766051Z","iopub.execute_input":"2025-06-02T15:41:40.766374Z","iopub.status.idle":"2025-06-02T15:41:40.802834Z","shell.execute_reply.started":"2025-06-02T15:41:40.766353Z","shell.execute_reply":"2025-06-02T15:41:40.801890Z"}},"outputs":[{"name":"stdout","text":"loading dataset from file...\n...done\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# TODO: create datasets\n# TODO: split train images/labels into train and val\n\ntrain_dataset = FERDataset(train_images, train_labels)\nval_dataset = FERDataset(val_images, val_labels)\ntest_dataset = FERDataset(test_images, test_labels)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MobileNetV2ForFacialExpressionRecognition(MobileNetV2PreTrainedModel):\n    \"\"\"\n    from MobileNetV2 for image classification\n    \"\"\"\n    def __init__(self, config):\n        super().__init__(config=config)\n\n        self.num_labels = 3\n        self.mobilenet_v2 = MobileNetV2Model(config)\n\n        last_hidden_size = self.mobilenet_v2.conv_1x1.convolution.out_channels\n\n        # Classifier head\n        self.dropout = torch.nn.Dropout(config.classifier_dropout_prob, inplace=True)\n        self.classifier = torch.nn.Linear(last_hidden_size, config.num_labels)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(self, pixel_values=None, output_hidden_states=None, labels=None):\n        outputs = self.mobilenet_v2(pixel_values, output_hidden_states=output_hidden_states)\n        pooled_output = outputs[1]\n        logits = self.classifier(self.dropout(pooled_output))\n\n        loss = None\n        if labels is not None:\n            self.config.problem_type = \"multi_label_classification\"\n            loss_fct = BCEWithLogitsLoss()\n            loss = loss_fct(logits, labels)\n        output = (logits,) + outputs[2:]\n        return ((loss,) + output) if loss is not None else output","metadata":{"trusted":true,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2025-06-02T14:56:17.020758Z","iopub.execute_input":"2025-06-02T14:56:17.021121Z","iopub.status.idle":"2025-06-02T14:56:17.029112Z","shell.execute_reply.started":"2025-06-02T14:56:17.021096Z","shell.execute_reply":"2025-06-02T14:56:17.028066Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"print('...creating MobileNetV2 model')\nmodel = MobileNetV2ForFacialExpressionRecognition(cfg).from_pretrained(PRETRAINED_MODEL)\nmodel.to(DEVICE)\nsummary(model, input_size=(BATCH_SIZE, 3, 48, 48))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T15:02:35.458973Z","iopub.execute_input":"2025-06-02T15:02:35.459314Z","iopub.status.idle":"2025-06-02T15:02:41.826569Z","shell.execute_reply.started":"2025-06-02T15:02:35.459288Z","shell.execute_reply":"2025-06-02T15:02:41.825127Z"},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":true}},"outputs":[{"name":"stdout","text":"...creating MobileNetV2 model\nRequirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (1.8.0)\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"====================================================================================================\nLayer (type:depth-idx)                             Output Shape              Param #\n====================================================================================================\nMobileNetV2ForFacialExpressionRecognition          [16, 1001]                --\n├─MobileNetV2Model: 1-1                            [16, 1792]                --\n│    └─MobileNetV2Stem: 2-1                        [16, 24, 24, 24]          --\n│    │    └─MobileNetV2ConvLayer: 3-1              [16, 48, 24, 24]          1,392\n│    │    └─MobileNetV2ConvLayer: 3-2              [16, 48, 24, 24]          528\n│    │    └─MobileNetV2ConvLayer: 3-3              [16, 24, 24, 24]          1,200\n│    └─ModuleList: 2-2                             --                        --\n│    │    └─MobileNetV2InvertedResidual: 3-4       [16, 32, 12, 12]          10,000\n│    │    └─MobileNetV2InvertedResidual: 3-5       [16, 32, 12, 12]          14,848\n│    │    └─MobileNetV2InvertedResidual: 3-6       [16, 48, 6, 6]            17,952\n│    │    └─MobileNetV2InvertedResidual: 3-7       [16, 48, 6, 6]            31,488\n│    │    └─MobileNetV2InvertedResidual: 3-8       [16, 48, 6, 6]            31,488\n│    │    └─MobileNetV2InvertedResidual: 3-9       [16, 88, 3, 3]            43,088\n│    │    └─MobileNetV2InvertedResidual: 3-10      [16, 88, 3, 3]            99,968\n│    │    └─MobileNetV2InvertedResidual: 3-11      [16, 88, 3, 3]            99,968\n│    │    └─MobileNetV2InvertedResidual: 3-12      [16, 88, 3, 3]            99,968\n│    │    └─MobileNetV2InvertedResidual: 3-13      [16, 136, 3, 3]           125,408\n│    │    └─MobileNetV2InvertedResidual: 3-14      [16, 136, 3, 3]           232,832\n│    │    └─MobileNetV2InvertedResidual: 3-15      [16, 136, 3, 3]           232,832\n│    │    └─MobileNetV2InvertedResidual: 3-16      [16, 224, 2, 2]           304,816\n│    │    └─MobileNetV2InvertedResidual: 3-17      [16, 224, 2, 2]           620,032\n│    │    └─MobileNetV2InvertedResidual: 3-18      [16, 224, 2, 2]           620,032\n│    │    └─MobileNetV2InvertedResidual: 3-19      [16, 448, 2, 2]           921,536\n│    └─MobileNetV2ConvLayer: 2-3                   [16, 1792, 2, 2]          --\n│    │    └─Conv2d: 3-20                           [16, 1792, 2, 2]          802,816\n│    │    └─BatchNorm2d: 3-21                      [16, 1792, 2, 2]          3,584\n│    │    └─ReLU6: 3-22                            [16, 1792, 2, 2]          --\n│    └─AdaptiveAvgPool2d: 2-4                      [16, 1792, 1, 1]          --\n├─Dropout: 1-2                                     [16, 1792]                --\n├─Linear: 1-3                                      [16, 1001]                1,794,793\n====================================================================================================\nTotal params: 6,110,569\nTrainable params: 6,110,569\nNon-trainable params: 0\nTotal mult-adds (Units.MEGABYTES): 543.73\n====================================================================================================\nInput size (MB): 0.44\nForward/backward pass size (MB): 117.95\nParams size (MB): 24.44\nEstimated Total Size (MB): 142.83\n===================================================================================================="},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"# TODO: loss function","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: compute accuracy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: train function","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: validate function","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: save best model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: plot epoch metrics function","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: train_loop","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: run training","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: plot train vs validation loss\n# TODO: plot train vs validation accuracy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: evaluate function (test data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: run evaluation (test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# TODO: export to ONNX","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}